{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['indices']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "import networkx as nx   # necessary for network structure plots\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from a text file\n",
    "\n",
    "The following cell loads the data stored in the text files \"train.txt\" and \"test.txt\". This results in two NumPy arrays with shapes 500x5 (train.txt) and 5000x5 (test.txt) - 500 and 5000 samples of the following 5 random variables:\n",
    "\n",
    "Column 0: S ... stress (false (0) or true (1))     \n",
    "Column 1: E ... easily catches cold (false (0) or true (1))  \n",
    "Column 2: G ... genetic disposition (false (0) or true (1))   \n",
    "Column 3: I ... increased blood pressure (false (0) or true (1))   \n",
    "Column 4: H ... heart attack (false (0) or true (1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "[[], [0], [], [2, 0], [3]]\n",
      "[0, 1, 2, 3, 4]\n",
      "[4, 3, 2, 1, 0]\n",
      "<type 'list'>\n",
      "[0, 1, 2, 3, 4]\n",
      "[[0], [1], [2], [3], [4]]\n",
      "(0, 1, 2, 3)\n",
      "(0, 1, 2, 4)\n",
      "(0, 1, 3, 4)\n",
      "(0, 2, 3, 4)\n",
      "(1, 2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset Y. Note: the file \"train.txt\" has to be in the same directory you started the ipython notebook server in\n",
    "Y = np.loadtxt('train.txt', dtype=int)\n",
    "\n",
    "# load the training dataset Z (used for last exercise). Again, the file has to be in correct directory.\n",
    "Z = np.loadtxt('test.txt', dtype=int)\n",
    "\n",
    "# row indices of the random variables\n",
    "_s_, _e_, _g_, _i_, _h_ = 0, 1, 2, 3, 4\n",
    "\n",
    "na = newaxis\n",
    "\n",
    "#print Z.shape[0]\n",
    "#print Z.shape[1]\n",
    "indices = [[0], [1, 0], [2], [3, 2, 0], [4, 3]]\n",
    "a = [0] * size(indices)\n",
    "b = [0] * size(indices)\n",
    "print size(b)\n",
    "i = 0\n",
    "for c in indices:\n",
    "    a[i] = c[0]\n",
    "    b[i] = c[1:] or []\n",
    "    i += 1\n",
    "    \n",
    "print a[0]\n",
    "print b\n",
    "print range(5)\n",
    "print range(5)[::-1]\n",
    "print type(range(5))\n",
    "print list(range(5))\n",
    "\n",
    "#j = 0\n",
    "#for d in range(5):\n",
    "for idx,d in enumerate(range(5)):\n",
    "    indices[idx] = [d]\n",
    "    #j += 1\n",
    "    \n",
    "print indices\n",
    "#indices2 = [[0], [1, 0], [2], [3, 2, 0], [4, 3]]\n",
    "#indices2[4].append(2)\n",
    "#print indices2\n",
    "#l = [0, 1, 2]\n",
    "#print indices2[2][0]\n",
    "l = [0, 1, 2, 3, 4]\n",
    "\n",
    "for c in it.combinations(l, 4):\n",
    "    print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for probability tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mle_1d(X, x):\n",
    "    \"\"\" \n",
    "    Probability distribution of a variable: P(VAR)\n",
    "    :param X: matrix array of boolean variables, in an integer shape (zeroes and ones)\n",
    "    :param x: column of the X matrix of which we want the probability distribution\n",
    "    :returns: array with the probability distribution of the required variable, given the array\n",
    "    \"\"\"\n",
    "    sm = X[:, x].sum()\n",
    "    num = X[:, x].shape[0] \n",
    "    return array([1 - sm / float(num), sm / float(num)])\n",
    "    \n",
    "def mle_2d(X, a, b):\n",
    "    \"\"\" \n",
    "    Conditional probability distribution of a variable, given another variable: P(VAR1 | VAR2)\n",
    "    :param X: matrix array of boolean variables, in an integer shape (zeroes and ones)\n",
    "    :param a: column of the X matrix of which we want the conditional probability distribution\n",
    "    :param b: conditioning column of the X matrix\n",
    "    :returns: array with the probability distribution of the required variable, given the array and the conditioning variable\n",
    "    \"\"\"\n",
    "    x = X[:, a]\n",
    "    y = X[:, b]\n",
    "    num = X.shape[0] \n",
    "    condA = where((x == 0) & (y == 0))[0].shape[0] / float(num)\n",
    "    condB = where((x == 0) & (y == 1))[0].shape[0] / float(num)\n",
    "    condC = where((x == 1) & (y == 0))[0].shape[0] / float(num)\n",
    "    condD = 1 - (condA + condB + condC)   \n",
    "    ab = array([[condA, condB], [condC, condD]])\n",
    "    return ab / ab.sum(1) / (ab / ab.sum(1)).sum(0)\n",
    "    \n",
    "\n",
    "def mle_3d(X, a, b, c):  \n",
    "    \"\"\" \n",
    "    Conditional probability distribution of a variable, given two other variables: P(VAR1 | VAR2, VAR3)\n",
    "    :param X: matrix array of boolean variables, in an integer shape (zeroes and ones)\n",
    "    :param a: column of the X matrix of which we want the conditional probability distribution\n",
    "    :param b: first conditioning column of the X matrix\n",
    "    :param c: second conditioning column of the X matrix\n",
    "    :returns: array with the probability distribution of the required variable, given the array and the conditioning variables\n",
    "    \"\"\"    \n",
    "    x = X[:, a]\n",
    "    y = X[:, b]\n",
    "    z = X[:, c]\n",
    "    num = X.shape[0] \n",
    "    condA = where((x == 0) & (y == 0) & (z == 0))[0].shape[0] / float(num)\n",
    "    condB = where((x == 0) & (y == 0) & (z == 1))[0].shape[0] / float(num)\n",
    "    condC = where((x == 0) & (y == 1) & (z == 0))[0].shape[0] / float(num)\n",
    "    condD = where((x == 0) & (y == 1) & (z == 1))[0].shape[0] / float(num)\n",
    "    condE = where((x == 1) & (y == 0) & (z == 0))[0].shape[0] / float(num)\n",
    "    condF = where((x == 1) & (y == 0) & (z == 1))[0].shape[0] / float(num)\n",
    "    condG = where((x == 1) & (y == 1) & (z == 0))[0].shape[0] / float(num)\n",
    "    condH = 1 - (condA + condB + condC + condD + condE + condF + condG)  \n",
    "    abc =  array([[[condA, condB], [condC, condD]], [[condE, condF], [condG, condH]]])   \n",
    "    bc = abc.sum(0)\n",
    "    return abc / bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mle(data, var, parents=None):\n",
    "    \"\"\"\n",
    "    Does maximum likelihood estimation of the (conditional) probability distribution\n",
    "    of a random variable given the data.\n",
    "    \n",
    "    :param data:    data to use for estimating the distribution\n",
    "    :param var:     index of the variable to estimate the distribution for\n",
    "    :param parents: parents (ids of given variables) of the variable in the Bayes net,\n",
    "                    given as list. If None or of length 0, the variable is assumed to have no parents.\n",
    "    :return:        Estimated (conditional) probability distribution of the random variable\n",
    "    \"\"\"\n",
    "    parents = parents or []\n",
    "    n_vars = len(parents) + 1\n",
    "    \n",
    "    Y = data[:, [var] + parents]\n",
    "    X = empty((2,) * n_vars)\n",
    "    \n",
    "    for vals in it.product([0, 1], repeat=n_vars):\n",
    "        X[vals] = np.sum(np.all(Y == vals, axis=1))\n",
    "        \n",
    "    return X / X.sum(0)\n",
    "\n",
    "\n",
    "def log_likelihood(data, dists, indices):\n",
    "    \"\"\"\n",
    "    Computes the log likelihood of the data given a model. A model is given as a list\n",
    "    of distributions (conditional probability tables) and corresponding indices describing\n",
    "    which dimension of each distribution corresponds to which random variable. E.g., \n",
    "    assume you have data for three random variables A, B and C given in alphabetical order.\n",
    "    If dists looks like [A_B, B, C_AB], indices would be [(0, 1), (1,), (2, 0, 1)].\n",
    "    \n",
    "    :param dists:   list of (conditional) probability tables for the random variables\n",
    "    :param indices: list of variable indices corresponding to the CPTs stored in 'dists'\n",
    "    :param data:    data to compute the log-likelihood of\n",
    "    :return:        log-likelihood of data given the model as encoded by dists and indices\n",
    "    \"\"\"\n",
    "    log_l = 0\n",
    "    for y in data:\n",
    "        for i in range(len(dists)):\n",
    "            idx = indices[i]\n",
    "            log_l += np.log(dists[i][tuple(y[idx])])\n",
    "            \n",
    "    return log_l\n",
    "\n",
    "\n",
    "def mean_log_likelihood(data, dists, indices):\n",
    "    \"\"\"\n",
    "    Computes the mean log likelihood of each data point in data.\n",
    "    :param dists:   list of (conditional) probability tables for the random variables\n",
    "    :param indices: list of variable indices corresponding to the CPTs stored in 'dists'\n",
    "    :param data:    data to compute the log-likelihood of\n",
    "    :return:        mean log-likelihood of data given the model as encoded by dists and indices\n",
    "    \"\"\"\n",
    "    return log_likelihood(data, dists, indices) / len(data)\n",
    "\n",
    "def bic(data, dists, indices):\n",
    "    \"\"\"\n",
    "    Computes the Bayesian information score\n",
    "    :param dists:   list of (conditional) probability tables for the random variables\n",
    "    :param indices: list of variable indices corresponding to the CPTs stored in 'dists'\n",
    "    :param data:    data to compute the log-likelihood of\n",
    "    :return:        Bayesian information score\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for c in indices:\n",
    "        if len(c) == 1:\n",
    "            i+= 1\n",
    "\n",
    "    return log_likelihood(data, dists, indices) - 0.5 * log(len(data)) * i\n",
    "\n",
    "def testScore(data, var_ord, score):\n",
    "    j = 0\n",
    "    for d in var_ord:\n",
    "        indices[j] = [d]\n",
    "        j += 1\n",
    "        \n",
    "    dists = learn_dists(data, indices)    \n",
    "    return score(data, dists, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_dists(data, indices):\n",
    "    a = [0] * size(indices)\n",
    "    b = [0] * size(indices)\n",
    "\n",
    "    for idx,c in enumerate(indices):\n",
    "        a[idx] = c[0]\n",
    "        b[idx] = c[1:] or []\n",
    "        \n",
    "    return [mle(data, a[0], b[0]), mle(data, a[1], b[1]), mle(data, a[2], b[2]), mle(data, a[3], b[3]), mle(data, a[4], b[4])]    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.A) Probability tables\n",
    "of P(S), P(G), P(I), P(E), P(H)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-986.34842317179766"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = mle_1d(Y,_s_)\n",
    "G = mle_1d(Y,_g_)\n",
    "I = mle_1d(Y,_i_)\n",
    "E = mle_1d(Y,_e_)\n",
    "H = mle_1d(Y,_h_)\n",
    "testScore(Y, score=bic, var_ord=range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.B) log likelihood of model 1 relative to the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood of model 1 relative to the train dataset is -970.81190292576616 .\n",
      "-970.811902926\n",
      "-1.94162380585\n",
      "-986.348423172\n",
      "-986.348423172\n"
     ]
    }
   ],
   "source": [
    "SEGIH1 = S[:, na, na, na, na] * E[na, :, na, na, na] * G[na, na, :, na, na] * I[na, na, na, :, na] * H[na, na, na, na, :] \n",
    "ll1r = log(SEGIH1[Y[:,0],Y[:,1],Y[:,2],Y[:,3],Y[:,4]]).sum()\n",
    "print 'The log likelihood of model 1 relative to the train dataset is ' + `ll1r` + ' .'\n",
    "dists1 = [mle(Y, 0), mle(Y, 1), mle(Y, 2), mle(Y, 3) , mle(Y, 4)]\n",
    "indices1 = [[0], [1], [2], [3], [4]]\n",
    "print log_likelihood(Y, dists1, indices1)\n",
    "print mean_log_likelihood(Y, dists1, indices1)\n",
    "print bic(Y, dists1, indices1)\n",
    "print testScore(Y, indices1, score = bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.A) Probability tables\n",
    "of P(E|S), P (I | G, S), and P (H | I) (The remaining ones are already given by 1.1.A))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E_S = mle_2d(Y, _e_, _s_)\n",
    "H_I = mle_2d(Y, _h_, _i_)\n",
    "I_GS = mle_3d(Y, _i_, _g_, _s_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.B) log likelihood of model 2 relative to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood of model 2 relative to the train dataset is -940.76297860888167 .\n",
      "-940.762978609\n",
      "-1.88152595722\n",
      "-946.977586707\n"
     ]
    }
   ],
   "source": [
    "SEGIH2 = S[:, na, na, na, na] * E_S.transpose((1, 0))[:, :, na, na, na] * G[na, na, :, na, na] * I_GS.transpose((2, 1, 0))[:, na, :, :, na] * H_I.transpose((1, 0))[na, na, na, :, :] \n",
    "ll2r = log(SEGIH2[Y[:,0],Y[:,1],Y[:,2],Y[:,3],Y[:,4]]).sum()\n",
    "print 'The log likelihood of model 2 relative to the train dataset is ' + `ll2r` + ' .'\n",
    "#dists2 = [mle(Y, 0), mle(Y, 1, [0]), mle(Y, 2), mle(Y, 3, [2, 0]), mle(Y, 4, [3])]\n",
    "indices2 = [[0], [1, 0], [2], [3, 2, 0], [4, 3]]\n",
    "\n",
    "\n",
    "    \n",
    "dists = learn_dists(Y, indices2)\n",
    "print log_likelihood(Y, dists2, indices2)\n",
    "print mean_log_likelihood(Y, dists2, indices2)\n",
    "print bic(Y, dists2, indices2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.A) Probability tables\n",
    "of P (S | G), P (E | S, I), and P (H | I, E) (The remaining ones are already given by 1.1.A) and 1.2.A)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S_G = mle_2d(Y, _s_, _g_)\n",
    "E_SI = mle_3d(Y, _e_, _s_, _i_)\n",
    "H_IE = mle_3d(Y, _h_, _i_, _e_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.B) log likelihood of model 3 relative to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood of model 3 relative to the train dataset is -939.73403209446235 .\n",
      "-939.734032094\n",
      "-1.87946806419\n",
      "1\n",
      "-942.841336144\n"
     ]
    }
   ],
   "source": [
    "SEGIH3 = S_G[:, na, :, na, na] * E_SI.transpose((1, 0, 2))[:, :, na, :, na] * G[na, na, :, na, na] * I_GS.transpose((2, 1, 0))[:, na, :, :, na] * H_IE.transpose((2, 1, 0))[na, :, na, :, :] \n",
    "ll3r = log(SEGIH3[Y[:,0],Y[:,1],Y[:,2],Y[:,3],Y[:,4]]).sum()\n",
    "print 'The log likelihood of model 3 relative to the train dataset is ' + `ll3r` + ' .'\n",
    "#dists3 = [G, S_G, E_SI, I_GS, H_IE]\n",
    "dists3 = [mle(Y, 2), mle(Y, 0, [2]), mle(Y, 1, [0, 3]), mle(Y, 3, [2, 0]), mle(Y, 4, [3, 1])]\n",
    "indices3 = [[2], [0, 2], [1, 0, 3], [3, 2, 0], [4, 3, 1]]\n",
    "print log_likelihood(Y, dists3, indices3)\n",
    "print mean_log_likelihood(Y, dists3, indices3)\n",
    "print bic(Y, dists3, indices3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.A) Compare training data\n",
    "We can state that minimizing the negative log likelihood is equivalent to maximize the likelihood estimation (MLE).  \n",
    "The model that maximizes the likelihood is clearly the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model for the training data is 3, \n",
      "because is the smallest, with a negative log likelihood of 939.73403209446235 .\n"
     ]
    }
   ],
   "source": [
    "trainll = abs(array([ll1r, ll2r, ll3r]))\n",
    "argmin(trainll)\n",
    "min(trainll)\n",
    "print 'The best model for the training data is ' + `argmin(trainll) + 1` + ', '\n",
    "print 'because is the smallest, with a negative log likelihood of ' + `min(trainll)` + ' .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model, the most complex of the three models, with a small training set, looks like to be the most effective.  \n",
    "The second model has a log likelihood very close to the third, leaving us wondering if all that complexity was really necessary.  \n",
    "The first model is the worst by far, probably due to ist excessive simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.B) Compare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9545.45920494  9216.89949044  9226.91241436]\n",
      "The best model for the test data is 2, \n",
      "because is the smallest, with a negative log likelihood of 9216.89949044479 .\n"
     ]
    }
   ],
   "source": [
    "testll = abs(array([log(SEGIH1[Z[:,0],Z[:,1],Z[:,2],Z[:,3],Z[:,4]]).sum(), \n",
    "                    log(SEGIH2[Z[:,0],Z[:,1],Z[:,2],Z[:,3],Z[:,4]]).sum(),\n",
    "                    log(SEGIH3[Z[:,0],Z[:,1],Z[:,2],Z[:,3],Z[:,4]]).sum()]))\n",
    "print testll\n",
    "argmin(testll)\n",
    "min(testll)\n",
    "print 'The best model for the test data is ' + `argmin(testll) + 1` + ', '\n",
    "print 'because is the smallest, with a negative log likelihood of ' + `min(testll)` + ' .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model, the most reasonable of the three models, with a large test set, is the most effective.  \n",
    "The third model, as feared, performs worse than the second, probably because of overfitting.  \n",
    "The first model is again the worst by far, again probably due to ist excessive simplicity.\n",
    "\n",
    "*Given the reasons in 3.A) and 3.B), I would choose the second model as the best for heart diseases.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.90909184099\n",
      "5\n",
      "-9566.75218792\n",
      "-1.84538248287\n",
      "2\n",
      "-9225.41668364\n",
      "-1.84337989809\n",
      "1\n",
      "-9231.17101095\n"
     ]
    }
   ],
   "source": [
    "print mean_log_likelihood(Z, dists1, indices1)\n",
    "print bic(Z, dists1, indices1)\n",
    "print mean_log_likelihood(Z, dists3, indices3)\n",
    "print bic(Z, dists2, indices2)\n",
    "print mean_log_likelihood(Z, dists2, indices2)\n",
    "print bic(Z, dists3, indices3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
