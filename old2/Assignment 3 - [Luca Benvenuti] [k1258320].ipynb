{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from a text file\n",
    "\n",
    "The following cell loads the data stored in the text files \"train.txt\" and \"test.txt\". This results in two NumPy arrays with shapes 500x5 (train.txt) and 5000x5 (test.txt) - 500 and 5000 samples of the following 5 random variables:\n",
    "\n",
    "Column 0: S ... stress (false (0) or true (1))     \n",
    "Column 1: E ... easily catches cold (false (0) or true (1))  \n",
    "Column 2: G ... genetic disposition (false (0) or true (1))   \n",
    "Column 3: I ... increased blood pressure (false (0) or true (1))   \n",
    "Column 4: H ... heart attack (false (0) or true (1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the training dataset Y. Note: the file \"train.txt\" has to be in the same directory you started the ipython notebook server in\n",
    "Y = np.loadtxt('train.txt', dtype=int)\n",
    "\n",
    "# load the training dataset Z (used for last exercise). Again, the file has to be in correct directory.\n",
    "Z = np.loadtxt('test.txt', dtype=int)\n",
    "\n",
    "# row indices of the random variables\n",
    "_s_, _e_, _g_, _i_, _h_ = 0, 1, 2, 3, 4\n",
    "\n",
    "na = newaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for probability tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mle_1d(X, x):\n",
    "    \"\"\" \n",
    "    Probability distribution of a variable: P(VAR)\n",
    "    :param X: matrix array of boolean variables, in an integer shape (zeroes and ones)\n",
    "    :param x: column of the X matrix of which we want the probability distribution\n",
    "    :returns: array with the probability distribution of the required variable, given the array\n",
    "    \"\"\"\n",
    "    sm = X[:, x].sum()\n",
    "    num = X[:, x].shape[0] \n",
    "    return array([1 - sm / float(num), sm / float(num)])\n",
    "    \n",
    "def mle_2d(X, a, b):\n",
    "    \"\"\" \n",
    "    Conditional probability distribution of a variable, given another variable: P(VAR1 | VAR2)\n",
    "    :param X: matrix array of boolean variables, in an integer shape (zeroes and ones)\n",
    "    :param a: column of the X matrix of which we want the conditional probability distribution\n",
    "    :param b: conditioning column of the X matrix\n",
    "    :returns: array with the probability distribution of the required variable, given the array and the conditioning variable\n",
    "    \"\"\"\n",
    "    x = X[:, a]\n",
    "    y = X[:, b]\n",
    "    num = X.shape[0] \n",
    "    condA = where((x == 0) & (y == 0))[0].shape[0] / float(num)\n",
    "    condB = where((x == 0) & (y == 1))[0].shape[0] / float(num)\n",
    "    condC = where((x == 1) & (y == 0))[0].shape[0] / float(num)\n",
    "    condD = 1 - (condA + condB + condC)   \n",
    "    ab = array([[condA, condB], [condC, condD]])\n",
    "    return ab / ab.sum(1) / (ab / ab.sum(1)).sum(0)\n",
    "    \n",
    "\n",
    "def mle_3d(X, a, b, c):  \n",
    "    \"\"\" \n",
    "    Conditional probability distribution of a variable, given two other variables: P(VAR1 | VAR2, VAR3)\n",
    "    :param X: matrix array of boolean variables, in an integer shape (zeroes and ones)\n",
    "    :param a: column of the X matrix of which we want the conditional probability distribution\n",
    "    :param b: first conditioning column of the X matrix\n",
    "    :param c: second conditioning column of the X matrix\n",
    "    :returns: array with the probability distribution of the required variable, given the array and the conditioning variables\n",
    "    \"\"\"    \n",
    "    x = X[:, a]\n",
    "    y = X[:, b]\n",
    "    z = X[:, c]\n",
    "    num = X.shape[0] \n",
    "    condA = where((x == 0) & (y == 0) & (z == 0))[0].shape[0] / float(num)\n",
    "    condB = where((x == 0) & (y == 0) & (z == 1))[0].shape[0] / float(num)\n",
    "    condC = where((x == 0) & (y == 1) & (z == 0))[0].shape[0] / float(num)\n",
    "    condD = where((x == 0) & (y == 1) & (z == 1))[0].shape[0] / float(num)\n",
    "    condE = where((x == 1) & (y == 0) & (z == 0))[0].shape[0] / float(num)\n",
    "    condF = where((x == 1) & (y == 0) & (z == 1))[0].shape[0] / float(num)\n",
    "    condG = where((x == 1) & (y == 1) & (z == 0))[0].shape[0] / float(num)\n",
    "    condH = 1 - (condA + condB + condC + condD + condE + condF + condG)  \n",
    "    abc =  array([[[condA, condB], [condC, condD]], [[condE, condF], [condG, condH]]])   \n",
    "    bc = abc.sum(0)\n",
    "    return abc / bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.A) Probability tables\n",
    "of P(S), P(G), P(I), P(E), P(H)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = mle_1d(Y,_s_)\n",
    "G = mle_1d(Y,_g_)\n",
    "I = mle_1d(Y,_i_)\n",
    "E = mle_1d(Y,_e_)\n",
    "H = mle_1d(Y,_h_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.B) log likelihood of model 1 relative to the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood of model 1 relative to the train dataset is -970.81190292576616 .\n"
     ]
    }
   ],
   "source": [
    "SEGIH1 = S[:, na, na, na, na] * E[na, :, na, na, na] * G[na, na, :, na, na] * I[na, na, na, :, na] * H[na, na, na, na, :] \n",
    "ll1r = log(SEGIH1[Y[:,0],Y[:,1],Y[:,2],Y[:,3],Y[:,4]]).sum()\n",
    "print 'The log likelihood of model 1 relative to the train dataset is ' + `ll1r` + ' .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.A) Probability tables\n",
    "of P(E|S), P (I | G, S), and P (H | I) (The remaining ones are already given by 1.1.A))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E_S = mle_2d(Y, _e_, _s_)\n",
    "H_I = mle_2d(Y, _h_, _i_)\n",
    "I_GS = mle_3d(Y, _i_, _g_, _s_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.B) log likelihood of model 2 relative to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood of model 2 relative to the train dataset is -940.76297860888167 .\n"
     ]
    }
   ],
   "source": [
    "SEGIH2 = S[:, na, na, na, na] * E_S.transpose((1, 0))[:, :, na, na, na] * G[na, na, :, na, na] * I_GS.transpose((2, 1, 0))[:, na, :, :, na] * H_I.transpose((1, 0))[na, na, na, :, :] \n",
    "ll2r = log(SEGIH2[Y[:,0],Y[:,1],Y[:,2],Y[:,3],Y[:,4]]).sum()\n",
    "print 'The log likelihood of model 2 relative to the train dataset is ' + `ll2r` + ' .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.A) Probability tables\n",
    "of P (S | G), P (E | S, I), and P (H | I, E) (The remaining ones are already given by 1.1.A) and 1.2.A)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mle_2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f661fc608047>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mS_G\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmle_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_s_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_g_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mE_SI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmle_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_e_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_s_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_i_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mH_IE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmle_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_h_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_i_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_e_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mle_2d' is not defined"
     ]
    }
   ],
   "source": [
    "S_G = mle_2d(Y, _s_, _g_)\n",
    "E_SI = mle_3d(Y, _e_, _s_, _i_)\n",
    "H_IE = mle_3d(Y, _h_, _i_, _e_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.B) log likelihood of model 3 relative to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S_G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f8b13c4713be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSEGIH3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mS_G\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mE_SI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mI_GS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mH_IE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mll3r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSEGIH3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'The log likelihood of model 3 relative to the train dataset is '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m`\u001b[0m\u001b[0mll3r\u001b[0m\u001b[1;33m`\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' .'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'S_G' is not defined"
     ]
    }
   ],
   "source": [
    "SEGIH3 = S_G[:, na, :, na, na] * E_SI.transpose((1, 0, 2))[:, :, na, :, na] * G[na, na, :, na, na] * I_GS.transpose((2, 1, 0))[:, na, :, :, na] * H_IE.transpose((2, 1, 0))[na, :, na, :, :] \n",
    "ll3r = log(SEGIH3[Y[:,0],Y[:,1],Y[:,2],Y[:,3],Y[:,4]]).sum()\n",
    "print 'The log likelihood of model 3 relative to the train dataset is ' + `ll3r` + ' .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.A) Compare training data\n",
    "We can state that minimizing the negative log likelihood is equivalent to maximize the likelihood estimation (MLE).  \n",
    "The model that maximizes the likelihood is clearly the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainll = abs(array([ll1r, ll2r, ll3r]))\n",
    "argmin(trainll)\n",
    "min(trainll)\n",
    "print 'The best model for the training data is ' + `argmin(trainll) + 1` + ', '\n",
    "print 'because is the smallest, with a negative log likelihood of ' + `min(trainll)` + ' .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model, the most complex of the three models, with a small training set, looks like to be the most effective.  \n",
    "The second model has a log likelihood very close to the third, leaving us wondering if all that complexity was really necessary.  \n",
    "The first model is the worst by far, probably due to ist excessive simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.B) Compare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9545.45920494  9216.89949044  9226.91241436]\n",
      "The best model for the test data is 2, \n",
      "because is the smallest, with a negative log likelihood of 9216.89949044479 .\n"
     ]
    }
   ],
   "source": [
    "testll = abs(array([log(SEGIH1[Z[:,0],Z[:,1],Z[:,2],Z[:,3],Z[:,4]]).sum(), \n",
    "                    log(SEGIH2[Z[:,0],Z[:,1],Z[:,2],Z[:,3],Z[:,4]]).sum(),\n",
    "                    log(SEGIH3[Z[:,0],Z[:,1],Z[:,2],Z[:,3],Z[:,4]]).sum()]))\n",
    "print testll\n",
    "argmin(testll)\n",
    "min(testll)\n",
    "print 'The best model for the test data is ' + `argmin(testll) + 1` + ', '\n",
    "print 'because is the smallest, with a negative log likelihood of ' + `min(testll)` + ' .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model, the most reasonable of the three models, with a large test set, is the most effective.  \n",
    "The third model, as feared, performs worse than the second, probably because of overfitting.  \n",
    "The first model is again the worst by far, again probably due to ist excessive simplicity.\n",
    "\n",
    "*Given the reasons in 3.A) and 3.B), I would choose the second model as the best for heart diseases.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
