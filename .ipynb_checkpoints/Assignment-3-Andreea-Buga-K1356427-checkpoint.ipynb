{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Parameter Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from a text file\n",
    "\n",
    "The following cell loads the data stored in the text files \"train.txt\" and \"test.txt\". This results in two NumPy arrays with shapes 500x5 (train.txt) and 5000x5 (test.txt) - 500 and 5000 samples of the following 5 random variables:\n",
    "\n",
    "Column 0: S ... stress (false (0) or true (1))     \n",
    "Column 1: E ... easily catches cold (false (0) or true (1))  \n",
    "Column 2: G ... genetic disposition (false (0) or true (1))   \n",
    "Column 3: I ... increased blood pressure (false (0) or true (1))   \n",
    "Column 4: H ... heart attack (false (0) or true (1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 5)\n",
      "(5000, 5)\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset Y. Note: the file \"train.txt\" has to be in the same directory you started the ipython notebook server in\n",
    "Y = np.loadtxt('train.txt', dtype=int)\n",
    "print Y.shape\n",
    "# print Y[:,0]\n",
    "# load the training dataset Z (used for last exercise). Again, the file has to be in correct directory.\n",
    "Z = np.loadtxt('test.txt', dtype=int)\n",
    "print Z.shape\n",
    "\n",
    "# row indices of the random variables\n",
    "row_index = {\"_s_\" : 0, \"_e_\" : 1, \"_g_\" : 2, \"_i_\" : 3, \"_h_\" : 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "For each shape of the probability table there is a corresponding sample taking the row indices and returning the probability table. Those will be further use for computing the probabilities for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Learning for 1D Probability Distribution Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mle_1d(var1_column):\n",
    "    \"\"\" \n",
    "    Calculates the probability distribution\n",
    "    of a variable given the training data\n",
    "    :param var1_column: column index of the random variable\n",
    "    :returns: Numpy array containing the probability distribution learned from the training data\n",
    "    \"\"\"\n",
    "    count_pos = 0\n",
    "    var1_val = Y[:, var1_column]\n",
    "    for i in range(0, var1_val.size):\n",
    "        if var1_val[i] == 1:\n",
    "            count_pos += 1\n",
    "            \n",
    "    return [double(var1_val.size-count_pos)/var1_val.size, double(count_pos)/var1_val.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mle_2d(var1_column, var2_column):\n",
    "    \"\"\" \n",
    "    Calculates the probability distribution of a conditional probability\n",
    "    P(var1|var2)\n",
    "    :param var1_column: column index of the first random variable\n",
    "    :param var2_column: column index of the second random variable\n",
    "    :returns: Numpy array containing the probability distribution learned from the training data\n",
    "    \"\"\"\n",
    "    if (var1_column == var2_column) :\n",
    "        return \"Invalid input parameters. Column indices should be different\"\n",
    "    var1_val = Y[:, var1_column]\n",
    "    var2_val = Y[:, var2_column]\n",
    "    \n",
    "    mle_pd = array([[0, 0],\n",
    "              [0, 0]])\n",
    "    for i in range(0, var1_val.size):\n",
    "        mle_pd [var1_val[i]][var2_val[i]] += 1 \n",
    "        \n",
    "    return array([[double(mle_pd[0][0])/np.sum(mle_pd[:, 0]), double(mle_pd[0][1])/np.sum(mle_pd[:, 1])],\n",
    "            [double(mle_pd[1][0])/np.sum(mle_pd[:, 0]), double(mle_pd[1][1])/np.sum(mle_pd[:, 1])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mle_3d(var1_column, var2_column, var3_column):\n",
    "    \"\"\" \n",
    "    Calculates the probability distribution of a conditional probability\n",
    "    P(var1|var2, var3)\n",
    "    :param var1_column: column index of the first random variable\n",
    "    :param var2_column: column index of the second random variable\n",
    "    :param var3_column: column index of the third random variable\n",
    "    :returns: Numpy array containing the probability distribution learned from the training data\n",
    "    \"\"\"\n",
    "    if (var1_column == var2_column or var1_column == var3_column or var2_column == var3_column) :\n",
    "        return \"Invalid input parameters. Column indices should be different\"\n",
    "    \n",
    "    var1_val = Y[:, var1_column]\n",
    "    var2_val = Y[:, var2_column]\n",
    "    var3_val = Y[:, var3_column]\n",
    "    \n",
    "    mle_pd = array ([[[0.0, 0.0],\n",
    "                    [0.0, 0.0]],\n",
    "                   [[0.0, 0.0],\n",
    "                   [0.0, 0.0]]])\n",
    "    \n",
    "    for i in range(0, var1_val.size):\n",
    "        mle_pd [var1_val[i]][var2_val[i]][var3_val[i]] += 1\n",
    "    \n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 2):\n",
    "            sum = np.sum(mle_pd[:, i, j])\n",
    "            for k in range (0, 2):\n",
    "                x = double(mle_pd[k][i][j])/sum\n",
    "                mle_pd[k][i][j] = double(mle_pd[k][i][j])/sum\n",
    "                 \n",
    "    return mle_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "\n",
    "Learn probability tables for the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution table for P(S):  [0.80600000000000005, 0.19400000000000001]\n",
      "Probability distribution table for P(E):  [0.90400000000000003, 0.096000000000000002]\n",
      "Probability distribution table for P(G):  [0.88400000000000001, 0.11600000000000001]\n",
      "Probability distribution table for P(I):  [0.83399999999999996, 0.16600000000000001]\n",
      "Probability distribution table for P(H):  [0.90000000000000002, 0.10000000000000001]\n"
     ]
    }
   ],
   "source": [
    "# Displaying information about the PDs of the first model\n",
    "S = mle_1d(row_index[\"_s_\"])\n",
    "E = mle_1d(row_index[\"_e_\"])\n",
    "G = mle_1d(row_index[\"_g_\"])\n",
    "I = mle_1d(row_index[\"_i_\"])\n",
    "H = mle_1d(row_index[\"_h_\"])\n",
    "print 'Probability distribution table for P(S): ', S\n",
    "print 'Probability distribution table for P(E): ', E\n",
    "print 'Probability distribution table for P(G): ', G\n",
    "print 'Probability distribution table for P(I): ', I\n",
    "print 'Probability distribution table for P(H): ', H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood of the first model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood of the first model on training data:  -970.811902926\n",
      "Log likelihood of the first model on test data:  -9545.45920494\n"
     ]
    }
   ],
   "source": [
    "def ml1_loglik(data):\n",
    "    l = 0\n",
    "    sample_no = data[:, 0].size\n",
    "    for i in range(0, sample_no):\n",
    "        row = data[i]\n",
    "        sample_prob = S[row[0]] * E[row[1]] * G[row[2]] * I[row[3]] * H[row[4]]\n",
    "        l += np.log(sample_prob)\n",
    "    return l\n",
    "\n",
    "print \"Log likelihood of the first model on training data: \", ml1_loglik(Y)\n",
    "print \"Log likelihood of the first model on test data: \", ml1_loglik(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "\n",
    "Learn probability tables for the second model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution table for P(S):  [0.80600000000000005, 0.19400000000000001]\n",
      "Probability distribution table for P(E|S):  [[ 0.92059553  0.83505155]\n",
      " [ 0.07940447  0.16494845]]\n",
      "Probability distribution table for P(G):  [0.88400000000000001, 0.11600000000000001]\n",
      "Probability distribution table for P(I|G, S):  [[[ 0.90960452  0.67045455]\n",
      "  [ 0.69387755  0.22222222]]\n",
      "\n",
      " [[ 0.09039548  0.32954545]\n",
      "  [ 0.30612245  0.77777778]]]\n",
      "Probability distribution table for P(H|I):  [[ 0.90647482  0.86746988]\n",
      " [ 0.09352518  0.13253012]]\n"
     ]
    }
   ],
   "source": [
    "# Displaying information about the PDs of the second model\n",
    "E_S = mle_2d(row_index[\"_e_\"], row_index[\"_s_\"])\n",
    "I_GS = mle_3d(row_index[\"_i_\"], row_index[\"_g_\"], row_index[\"_s_\"])\n",
    "H_I = mle_2d(row_index[\"_h_\"], row_index[\"_i_\"])\n",
    "print 'Probability distribution table for P(S): ', S\n",
    "print 'Probability distribution table for P(E|S): ', E_S\n",
    "print 'Probability distribution table for P(G): ', G\n",
    "print 'Probability distribution table for P(I|G, S): ', I_GS\n",
    "print 'Probability distribution table for P(H|I): ', H_I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood of the second model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood of the second model on training data:  -940.762978609\n",
      "Log likelihood of the second model on test data:  -9216.89949044\n"
     ]
    }
   ],
   "source": [
    "def ml2_loglik(data):\n",
    "    l = 0\n",
    "    sample_no = data[:, 0].size\n",
    "    for i in range(0, sample_no):\n",
    "        row = data[i]\n",
    "        sample_prob = S[row[0]] * E_S[row[1]][row[0]] * G[row[2]] * I_GS[row[3]][row[2]][row[0]] * H_I[row[4]][row[3]]\n",
    "        l += np.log(sample_prob)\n",
    "    return l\n",
    "\n",
    "print \"Log likelihood of the second model on training data: \",ml2_loglik(Y)\n",
    "print \"Log likelihood of the second model on test data: \",ml2_loglik(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "Learn probability tables for the third model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution table for P(G):  [0.88400000000000001, 0.11600000000000001]\n",
      "Probability distribution table for P(S|G):  [[ 0.80090498  0.84482759]\n",
      " [ 0.19909502  0.15517241]]\n",
      "Probability distribution table for P(E|S, I):  [[[ 0.91853933  0.93617021]\n",
      "  [ 0.81967213  0.86111111]]\n",
      "\n",
      " [[ 0.08146067  0.06382979]\n",
      "  [ 0.18032787  0.13888889]]]\n",
      "Probability distribution table for P(I|G, S):  [[[ 0.90960452  0.67045455]\n",
      "  [ 0.69387755  0.22222222]]\n",
      "\n",
      " [[ 0.09039548  0.32954545]\n",
      "  [ 0.30612245  0.77777778]]]\n",
      "Probability distribution table for P(H|I, E):  [[[ 0.9071618  0.9      ]\n",
      "  [ 0.88       0.75     ]]\n",
      "\n",
      " [[ 0.0928382  0.1      ]\n",
      "  [ 0.12       0.25     ]]]\n"
     ]
    }
   ],
   "source": [
    "# Displaying information about the PDs of the third model\n",
    "S_G = mle_2d(row_index[\"_s_\"], row_index[\"_g_\"])\n",
    "E_SI = mle_3d(row_index[\"_e_\"], row_index[\"_s_\"], row_index[\"_i_\"])\n",
    "H_IE = mle_3d(row_index[\"_h_\"], row_index[\"_i_\"], row_index[\"_e_\"] )\n",
    "print 'Probability distribution table for P(G): ', G\n",
    "print 'Probability distribution table for P(S|G): ', S_G\n",
    "print 'Probability distribution table for P(E|S, I): ', E_SI\n",
    "print 'Probability distribution table for P(I|G, S): ', I_GS\n",
    "print 'Probability distribution table for P(H|I, E): ', H_IE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood of the second model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood of the third model on training data:  -939.734032094\n",
      "Log likelihood of the third model on test data:  -9226.91241436\n"
     ]
    }
   ],
   "source": [
    "def ml3_loglik(data):\n",
    "    l = 0\n",
    "    sample_no = data[:, 0].size\n",
    "    for i in range(0, sample_no):\n",
    "        row = data[i]\n",
    "        sample_prob = G[row[2]] * S_G[row[0]][row[2]] * E_SI[row[1]][row[0]][row[3]] * I_GS[row[3]][row[2]][row[0]] * H_IE[row[4]][row[3]][row[1]]      \n",
    "        l += np.log(sample_prob)\n",
    "    return l\n",
    "\n",
    "print \"Log likelihood of the third model on training data: \", ml3_loglik(Y)\n",
    "print \"Log likelihood of the third model on test data: \", ml3_loglik(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "\n",
    "### Observation\n",
    "\n",
    "Normally, it is desired to maximize the log-likelihood of a model. However, log likelihood gives negative values (probabilities are always smaller than 1). For this reason we either look at the biggest negative value of the log likelihood or at the smallest absolute value of the log likelihood. \n",
    "\n",
    "## Part A - Comparison of the log likelihood of models on the training data\n",
    "\n",
    "The values of the log likelihood of the models reveal how well the models fit the data. In case of the models build for the training data, they are built in such way so that they can fit it better. However, depending on the complexity of the model, performances differ. \n",
    "More complex models take into account dependencies contained in the training data and perform better on it. Simpler models neglect such dependencies and usually perform worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood of the first model on training data:  -970.811902926\n",
      "Log likelihood of the second model on training data:  -940.762978609\n",
      "Log likelihood of the third model on training data:  -939.734032094\n"
     ]
    }
   ],
   "source": [
    "print \"Log likelihood of the first model on training data: \", ml1_loglik(Y)\n",
    "print \"Log likelihood of the second model on training data: \",ml2_loglik(Y)\n",
    "print \"Log likelihood of the third model on training data: \", ml3_loglik(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more complex the model, the better the results. Therefore, model 3 fits the best the observation data, model 2 follows it closely, while the simplest model (model 1) is quite bad. The good results of the third model are usually explained by its complexity constructed especially from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Comparison of the log likelihood of models on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood of the third model on test data:  -9545.45920494\n",
      "Log likelihood of the second model on test data:  -9216.89949044\n",
      "Log likelihood of the third model on test data:  -9226.91241436\n"
     ]
    }
   ],
   "source": [
    "print \"Log likelihood of the third model on test data: \", ml1_loglik(Z)\n",
    "print \"Log likelihood of the second model on test data: \",ml2_loglik(Z)\n",
    "print \"Log likelihood of the third model on test data: \", ml3_loglik(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, model 3 now performed worse results. This is explained due to its overfitting to the training data. The best results are exposed by the second model which is not simple, but not very complex also and which tries to find dependencies on the data. First model is too simple and does not catch relevant dependencies in the data. Its performance is extremely low in comparison with the other two models.\n",
    "\n",
    "I consider the second model to be the best option for future analysis of medical data. It is not too complex to overfit data, and not too simple such that it will not catch important dependencies. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
