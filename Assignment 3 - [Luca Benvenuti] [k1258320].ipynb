{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from a text file\n",
    "\n",
    "The following cell loads the data stored in the text files \"train.txt\" and \"test.txt\". This results in two NumPy arrays with shapes 500x5 (train.txt) and 5000x5 (test.txt) - 500 and 5000 samples of the following 5 random variables:\n",
    "\n",
    "Column 0: S ... stress (false (0) or true (1))     \n",
    "Column 1: E ... easily catches cold (false (0) or true (1))  \n",
    "Column 2: G ... genetic disposition (false (0) or true (1))   \n",
    "Column 3: I ... increased blood pressure (false (0) or true (1))   \n",
    "Column 4: H ... heart attack (false (0) or true (1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 5)\n",
      "(5000, 5)\n"
     ]
    }
   ],
   "source": [
    "# load the training dataset Y. Note: the file \"train.txt\" has to be in the same directory you started the ipython notebook server in\n",
    "Y = np.loadtxt('train.txt', dtype=int)\n",
    "print Y.shape\n",
    "\n",
    "# load the training dataset Z (used for last exercise). Again, the file has to be in correct directory.\n",
    "Z = np.loadtxt('test.txt', dtype=int)\n",
    "print Z.shape\n",
    "\n",
    "# row indices of the random variables\n",
    "_s_, _e_, _g_, _i_, _h_ = 0, 1, 2, 3, 4\n",
    "\n",
    "na=newaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for probability tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mle_1d(X, x):\n",
    "    sm = X[:, x].sum()\n",
    "    num = X[:, x].shape[0] \n",
    "    #print sm\n",
    "    if num == 0.0: \n",
    "        return array([NaN, NaN]) #suggestion from T.A.\n",
    "    else:\n",
    "        return array([1 - sm / float(num), sm / float(num)])\n",
    "    \n",
    "def mle_2d(X, a, b):  \n",
    "    x = X[:, a]\n",
    "    y = X[:, b]\n",
    "    num = X.shape[0] \n",
    "    condA = where((x == 0) & (y == 0))[0].shape[0] / float(num)\n",
    "    condB = where((x == 0) & (y == 1))[0].shape[0] / float(num)\n",
    "    condC = where((x == 1) & (y == 0))[0].shape[0] / float(num)\n",
    "    condD = 1 - (condA + condB + condC)   \n",
    "    ab = array([[condA, condB], [condC, condD]])\n",
    "    return ab / ab.sum(1) / (ab / ab.sum(1)).sum(0)\n",
    "    \n",
    "\n",
    "def mle_3d(X, a, b, c):  \n",
    "    x = X[:, a]\n",
    "    y = X[:, b]\n",
    "    z = X[:, c]\n",
    "    num = X.shape[0] \n",
    "    condA = where((x == 0) & (y == 0) & (z == 0))[0].shape[0] / float(num)\n",
    "    condB = where((x == 0) & (y == 0) & (z == 1))[0].shape[0] / float(num)\n",
    "    condC = where((x == 0) & (y == 1) & (z == 0))[0].shape[0] / float(num)\n",
    "    condD = where((x == 0) & (y == 1) & (z == 1))[0].shape[0] / float(num)\n",
    "    condE = where((x == 1) & (y == 0) & (z == 0))[0].shape[0] / float(num)\n",
    "    condF = where((x == 1) & (y == 0) & (z == 1))[0].shape[0] / float(num)\n",
    "    condG = where((x == 1) & (y == 1) & (z == 0))[0].shape[0] / float(num)\n",
    "    condH = 1 - (condA + condB + condC + condD + condE + condF + condG)  \n",
    "    abc =  array([[[condA, condB], [condC, condD]], [[condE, condF], [condG, condH]]])   \n",
    "    bc = abc.sum(0)\n",
    "    return abc / bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.A) Probability tables\n",
    "of P(S), P(G), P(I), P(E), P(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = mle_1d(Y,_s_)\n",
    "G = mle_1d(Y,_g_)\n",
    "I = mle_1d(Y,_i_)\n",
    "E = mle_1d(Y,_e_)\n",
    "H = mle_1d(Y,_h_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.B) log likelihood of model 1 relative to the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-970.811902926\n"
     ]
    }
   ],
   "source": [
    "SEGIH1 = S[:, na, na, na, na] * E[na, :, na, na, na] * G[na, na, :, na, na] * I[na, na, na, :, na] * H[na, na, na, na, :] \n",
    "ll1r = log(SEGIH1[Y[:,0],Y[:,1],Y[:,2],Y[:,3],Y[:,4]]).sum()\n",
    "print ll1r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.A) Probability tables\n",
    "of P(E|S), P (I | G, S), and P (H | I) (The remaining ones are already given by 1A))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E_S = mle_2d(Y, _e_, _s_)\n",
    "H_I = mle_2d(Y, _h_, _i_)\n",
    "I_GS = mle_3d(Y, _i_, _g_, _s_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.B) log likelihood of model 2 relative to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-940.762978609\n"
     ]
    }
   ],
   "source": [
    "SEGIH2 = S[:, na, na, na, na] * E_S.transpose((1, 0))[:, :, na, na, na] * G[na, na, :, na, na] * I_GS.transpose((2, 1, 0))[:, na, :, :, na] * H_I.transpose((1, 0))[na, na, na, :, :] \n",
    "ll2r = log(SEGIH2[Y[:,0],Y[:,1],Y[:,2],Y[:,3],Y[:,4]]).sum()\n",
    "print ll2r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.A) Probability tables\n",
    "of P (S | G), P (E | S, I), and P (H | I, E) (The remaining ones are already given by 1A) and 2A)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_G = mle_2d(Y, _s_, _g_)\n",
    "E_SI = mle_3d(Y, _e_, _s_, _i_)\n",
    "H_IE = mle_3d(Y, _h_, _i_, _e_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.B) log likelihood of model 3 relative to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-939.734032094\n"
     ]
    }
   ],
   "source": [
    "SEGIH3 = S_G[:, na, :, na, na] * E_SI.transpose((1, 0, 2))[:, :, na, :, na] * G[na, na, :, na, na] * I_GS.transpose((2, 1, 0))[:, na, :, :, na] * H_IE.transpose((2, 1, 0))[na, :, na, :, :] \n",
    "ll3r = log(SEGIH3[Y[:,0],Y[:,1],Y[:,2],Y[:,3],Y[:,4]]).sum()\n",
    "print ll3r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.A\n",
    "Model 3 is the best for training :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.B Compare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9545.45920494\n",
      "-9216.89949044\n",
      "-9226.91241436\n"
     ]
    }
   ],
   "source": [
    "print log(SEGIH1[Z[:,0],Z[:,1],Z[:,2],Z[:,3],Z[:,4]]).sum()\n",
    "print log(SEGIH2[Z[:,0],Z[:,1],Z[:,2],Z[:,3],Z[:,4]]).sum()\n",
    "print log(SEGIH3[Z[:,0],Z[:,1],Z[:,2],Z[:,3],Z[:,4]]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time model 2 wins. Model 3 is probably overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
